{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions\n",
    "***\n",
    "This scripts contains reusable functions for data science (Read sub-sections of each functions for detailed information):\n",
    "> \n",
    "**```Key Highlights ```**\n",
    "\n",
    "    0. Loading basic libraries  \n",
    "    1. Functions for \n",
    "        * Read dataset based on file extensions\n",
    "        * Correlation Matrix for categorical features\n",
    "        * Correlation Matrix for all features present in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Step0: Load basic libraries ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make cells interactive\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\",100)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import scipy.stats as ss\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 1: Read Dataset ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pandas(file_name): \n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \"\"\"Read DataFrame based on the file extension. This function is used when the file is in a standard format.\n",
    "    Various file types are supported (.csv, .json, .jsonl, .data, .tsv, .xls, .xlsx, .xpt, .sas7bdat, .parquet)\n",
    "\n",
    "    Args:\n",
    "        file_name: the file to read\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    Eg: type1-\n",
    "        path= \"C:/users/data input/\"\n",
    "        filename= \"Sample_data.csv\"\n",
    "        df = read_pandas(f'{path}{filename}')\n",
    "        \n",
    "        type2-\n",
    "        filename= \"C:/users/data input/Sample_data.csv\"\n",
    "        df = read_pandas(f'{filename})\n",
    "        \n",
    "        type3-\n",
    "        filename= \"C:/users/data input/Sample_data.csv\"\n",
    "        df = read_pandas(filename)\n",
    "        \n",
    "    Notes:\n",
    "        This function is based on pandas IO tools:\n",
    "        https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "        https://pandas.pydata.org/pandas-docs/stable/reference/io.html\n",
    "\n",
    "        This function is not intended to be flexible or complete. The main use case is to be able to read files without\n",
    "        user input, which is currently used in the editor integration. For more advanced use cases, the user should load\n",
    "        the DataFrame in code.\n",
    "    \"\"\"\n",
    "    extension = Path(file_name).suffix.lower()\n",
    "    if extension == \".json\":\n",
    "        df = pd.read_json(str(file_name))\n",
    "    elif extension == \".jsonl\":\n",
    "        df = pd.read_json(str(file_name), lines=True)\n",
    "    elif extension == \".dta\":\n",
    "        df = pd.read_stata(str(file_name))\n",
    "    elif extension == \".tsv\":\n",
    "        df = pd.read_csv(str(file_name), sep=\"\\t\")\n",
    "    elif extension in [\".xls\", \".xlsx\"]:\n",
    "        df = pd.read_excel(str(file_name))\n",
    "    elif extension in [\".hdf\", \".h5\"]:\n",
    "        df = pd.read_hdf(str(file_name))\n",
    "    elif extension in [\".sas7bdat\", \".xpt\"]:\n",
    "        df = pd.read_sas(str(file_name))\n",
    "    elif extension == \".parquet\":\n",
    "        df = pd.read_parquet(str(file_name))\n",
    "    elif extension in [\".pkl\", \".pickle\"]:\n",
    "        df = pd.read_pickle(str(file_name))\n",
    "    else:\n",
    "        if extension != \".csv\":\n",
    "            warn_read(extension)\n",
    "\n",
    "        df = pd.read_csv(str(file_name))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 2: Reorder Columns in PD DataFrame ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderColumns(df,neworder):\n",
    "    \"\"\" Reorder columns based on the names\n",
    "    args: \n",
    "        df- dataframe\n",
    "        neworder- list/vector of columns names which you want at start\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    E.g.:\n",
    "        df= reorderColumns(df,['Index','TableID'])\n",
    "    \"\"\" \n",
    "    cols = df.columns.values\n",
    "    newCols= list(neworder)\n",
    "    newCols.extend([item for item in cols if item not in newCols])\n",
    "    df=df[newCols]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 3: Correlation Matrix only for categorical features ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate correlatiob between categorical variables\n",
    "\n",
    "def cramers_v1(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "\n",
    "#Generate correlation matrix using cramers_v1 function\n",
    "def cat_correl_matrix(df, col_list = None):\n",
    "    \"\"\" calculate correlation matrix using Cramers V statistic based on cramers_v1 for categorial-categorial association.\n",
    "    args:\n",
    "        df - dataframe\n",
    "        col_list - varibale list for which correlation to be calculated\n",
    "    E.g. \n",
    "        cat_correl_matrix(data,['var1','var2'])\n",
    "                        or\n",
    "        cat_correl_matrix(data)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if (col_list == None ):\n",
    "    #select features for which correlations needs to be calculated\n",
    "     cat_col = df.select_dtypes(['category']).columns\n",
    "\n",
    "    else : \n",
    "        cat_col = col_list\n",
    "\n",
    "    if (len(cat_col) == 0) : \n",
    "        return (print('* Categoical columns are not present in input dataset.'+ str('\\n')+ \n",
    "                      '* Please change datatypes to categorical for required features'))\n",
    "    else :\n",
    "\n",
    "        correl_mat =pd.DataFrame(data='',index=cat_col,columns=cat_col)\n",
    "        #calculating correlation matrix\n",
    "        for i in range(len(cat_col)):\n",
    "            for j in range(i):\n",
    "                confusion_matrix = pd.crosstab(df[cat_col[i]], df[cat_col[j]]).as_matrix()\n",
    "                correl_mat.iloc[i,j]= round(100*cramers_v1(confusion_matrix),2)\n",
    "        #Output \n",
    "        print(\"Correlation Matrix of categorical variables are:-\")\n",
    "        return correl_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 4: Correlation Matrix and heatmap for all features in dataset ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correaltion Matrix and heatmap \n",
    "\n",
    "def convert(data, to):\n",
    "    converted = None\n",
    "    if to == 'array':\n",
    "        if isinstance(data, np.ndarray):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values\n",
    "        elif isinstance(data, list):\n",
    "            converted = np.array(data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            converted = data.as_matrix()\n",
    "    elif to == 'list':\n",
    "        if isinstance(data, list):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = data.tolist()\n",
    "    elif to == 'dataframe':\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            converted = data\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data conversion: {}\".format(to))\n",
    "    if converted is None:\n",
    "        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n",
    "    else:\n",
    "        return converted\n",
    "    \n",
    "def conditional_entropy(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the conditional entropy of x given y: S(x|y)\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Conditional_entropy\n",
    "    :param x: list / NumPy ndarray / Pandas Series\n",
    "        A sequence of measurements\n",
    "    :param y: list / NumPy ndarray / Pandas Series\n",
    "        A sequence of measurements\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    # entropy of x given y\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x,y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0.0\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y/p_xy)\n",
    "    return entropy\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x\n",
    "\n",
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = numerator/denominator\n",
    "    return eta\n",
    "\n",
    "def associations(dataset, nominal_columns=None, mark_columns=False, theil_u=False, plot=True,\n",
    "                          return_results = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the correlation/strength-of-association of features in data-set with both categorical (eda_tools) and\n",
    "    continuous features using:\n",
    "     - Pearson's R for continuous-continuous cases\n",
    "     - Correlation Ratio for categorical-continuous cases\n",
    "     - Cramer's V or Theil's U for categorical-categorical cases\n",
    "    :param dataset: NumPy ndarray / Pandas DataFrame\n",
    "        The data-set for which the features' correlation is computed\n",
    "    :param nominal_columns: string / list / NumPy ndarray\n",
    "        Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n",
    "        columns are categorical, or None (default) to state none are categorical\n",
    "    :param mark_columns: Boolean (default: False)\n",
    "        if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n",
    "        continuous), as provided by nominal_columns\n",
    "    :param theil_u: Boolean (default: False)\n",
    "        In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n",
    "    :param plot: Boolean (default: True)\n",
    "        If True, plot a heat-map of the correlation matrix\n",
    "    :param return_results: Boolean (default: False)\n",
    "        If True, the function will return a Pandas DataFrame of the computed associations\n",
    "    :param kwargs:\n",
    "        Arguments to be passed to used function and methods\n",
    "    :return: Pandas DataFrame\n",
    "        A DataFrame of the correlation/strength-of-association between all features\n",
    "    :E.g. \n",
    "    results = associations(df,nominal_columns='all',return_results=True)\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = convert(dataset, 'dataframe')\n",
    "    columns = dataset.columns\n",
    "    if nominal_columns is None:\n",
    "        nominal_columns = list()\n",
    "    elif nominal_columns == 'all':\n",
    "        nominal_columns = columns\n",
    "    corr = pd.DataFrame(index=columns, columns=columns)\n",
    "    for i in range(0,len(columns)):\n",
    "        for j in range(i,len(columns)):\n",
    "            if i == j:\n",
    "                corr[columns[i]][columns[j]] = 1.0\n",
    "            else:\n",
    "                if columns[i] in nominal_columns:\n",
    "                    if columns[j] in nominal_columns:\n",
    "                        if theil_u:\n",
    "                            corr[columns[j]][columns[i]] = theils_u(dataset[columns[i]],dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = theils_u(dataset[columns[j]],dataset[columns[i]])\n",
    "                        else:\n",
    "                            cell = cramers_v(dataset[columns[i]],dataset[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                    else:\n",
    "                        cell = correlation_ratio(dataset[columns[i]], dataset[columns[j]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "                else:\n",
    "                    if columns[j] in nominal_columns:\n",
    "                        cell = correlation_ratio(dataset[columns[j]], dataset[columns[i]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "                    else:\n",
    "                        cell, _ = ss.pearsonr(dataset[columns[i]], dataset[columns[j]])\n",
    "                        corr[columns[i]][columns[j]] = cell\n",
    "                        corr[columns[j]][columns[i]] = cell\n",
    "    corr.fillna(value=np.nan, inplace=True)\n",
    "    if mark_columns:\n",
    "        marked_columns = ['{} (nom)'.format(col) if col in nominal_columns else '{} (con)'.format(col) for col in columns]\n",
    "        corr.columns = marked_columns\n",
    "        corr.index = marked_columns\n",
    "    if plot:\n",
    "        plt.figure(figsize=(20,20))#kwargs.get('figsize',None))\n",
    "        sns.heatmap(corr, annot=kwargs.get('annot',True), fmt=kwargs.get('fmt','.2f'), cmap='coolwarm')\n",
    "        plt.show()\n",
    "    if return_results:\n",
    "        return corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 5: Change integer into ordinal format ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis function convert integer into it's ordinal value\\n--dependencie import math\\n--args: integer number\\nEg: \\n    syntax- ordinal(10)\\n    output- '10th'\\n    or\\n    syntax- print([ordinal(n) for n in range(1,5)]) #for \\n    output- ['1st', '2nd', '3rd', '4th']\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ordinal numbers replacement\n",
    "\n",
    "import math\n",
    "\"\"\"\n",
    "This function convert integer into it's ordinal value\n",
    "--dependencie import math\n",
    "--args: integer number\n",
    "Eg: \n",
    "    syntax- ordinal(10)\n",
    "    output- '10th'\n",
    "    or\n",
    "    syntax- print([ordinal(n) for n in range(1,5)]) #for \n",
    "    output- ['1st', '2nd', '3rd', '4th']\n",
    "\"\"\"\n",
    "ordinal= lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(math.floor(n/10)%10!=1)*(n%10<4)*n%10::4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 6: Generate Date for a period ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateGenerator (startDate,endDate,freq='D',missingDays=None):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        startDate- period Start Date\n",
    "        endDate- period end date\n",
    "        freq- Interval at which dates to be generated, default 'D' for daily\n",
    "        missingDays - List of day which user wants to omit\n",
    "    eg:\n",
    "        dateGenerator('1-1-2017','12-31-2018',freq='3M',missingDays=['Saturday'])\n",
    "                                        or\n",
    "        dateGenerator('1-1-2017','12-31-2018',freq='3m',missingDays=['Saturday'])\n",
    "        dateGenerator('1-1-2017','12-31-2018')\n",
    "    Output:\n",
    "                0\n",
    "        0\t2017-01-02\n",
    "        1\t2017-01-03\n",
    "    \"\"\"\n",
    "    date = pd.date_range(start=startDate, end=endDate, freq=freq)\n",
    "    \n",
    "    if (missingDays is None):\n",
    "        return(date) \n",
    "    else : \n",
    "        return(date[~date.strftime('%A').isin(missingDays)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 7: Basic Distribution Bar Grpah ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barGraph(df,path):\n",
    "    for column in df:\n",
    "        \"\"\"\n",
    "        Creats bar graphs of all columns in a dataset\n",
    "        args:\n",
    "            df - dataFrame\n",
    "            path- location where graphs will be saved\n",
    "        \"\"\"\n",
    "\n",
    "        # create a figure and axis \n",
    "        _=fig, ax = plt.subplots(figsize=(8, 4));\n",
    "        _=data1 = df[column].value_counts()/len(df)\n",
    "        _=ax.bar(data1.index, data1.values,width=0.4, color = 'blue') \n",
    "        _=ax.set_title(str(column), y=0.9)\n",
    "        _=ax.set_ylim([0,1])\n",
    "        _=ax.set_ylabel('Distribution %')\n",
    "        _=ax.set_xticks(np.arange(len(data1.index)))\n",
    "\n",
    "\n",
    "        for rect in ax.bar(data1.index, data1.values,width=0.4):\n",
    "            _=height =rect.get_height()\n",
    "            _=ax.text(rect.get_x() + rect.get_width()/2.0, height, f'{np.round(100*height,1)}%' , \\\n",
    "                      ha='center', va='bottom',fontsize=10)\n",
    "            #fontweight='bold'\n",
    "        plt.savefig(f'{path}\\\\{column}.png', dpi=300, format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 8: Paste equivalent of R ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste(List, sep=''):\n",
    "    \"\"\"\n",
    "    Creates a str object\n",
    "    args:\n",
    "        List - list or range \n",
    "        sep- seperator\n",
    "    e.g: \n",
    "        Input- l=['a','b','c','d','e']\n",
    "        Syntax- paste(l)\n",
    "        Output- 'abcde'\n",
    "        \n",
    "    \"\"\"\n",
    "    strCombn =str()\n",
    "    for i in range(len(List)):\n",
    "        temp= f'{List[i]}'\n",
    "        if (i==0):\n",
    "            strCombn= temp\n",
    "        else:\n",
    "            strCombn = f'{strCombn}{sep}{temp}'\n",
    "        \n",
    "    return strCombn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 9: Data Frame columns types and Null % view ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NullSummary(df):\n",
    "    \"\"\"\n",
    "    Gives info on columns types and number of null values\n",
    "    args:\n",
    "        dataFrame\n",
    "    \"\"\"\n",
    "    print('Dataframe dimensions:', df.shape)\n",
    "    tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n",
    "    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'missing values'}))\n",
    "    tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "    return(tab_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 10: summary of groupby object ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGrpStats(group):\n",
    "    \"\"\"\n",
    "    function that extract statistical parameters from a grouby objet\n",
    "    args:\n",
    "        group variables\n",
    "    syntax:\n",
    "        global_stats = df['AMOUNT'].groupby(df['ORG_NAME']).apply(get_stats).unstack()\n",
    "    \"\"\"\n",
    "    return {'min': group.min(), 'max': group.max(),\n",
    "            'count': group.count(), 'mean': group.mean(), 'sum':group.sum()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``` Function 11: ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Analysis of dataframe\n",
    "import pandas_profiling\n",
    "df.profile_report(style={'full_width':True}) #output in notebook\n",
    "df.profile_report().to_file(output_file=\"dfProfileReport.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
